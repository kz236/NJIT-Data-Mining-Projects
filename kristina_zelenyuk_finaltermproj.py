# -*- coding: utf-8 -*-
"""Kristina_Zelenyuk_FinalTermProj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e2LBTcyZ5vqkgtsGqbGyOLzfN8qiLgia

Import necessary packages, import Breast Cancer data from sklearn.
"""

import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix
from sklearn.metrics import brier_score_loss
from sklearn.metrics import roc_auc_score

"""create row title array for metrics"""

metricnames=['Model Type', 'Precision', 'Recall', 'True Negative Rate', 'F1 Score', 'Accuracy', 'Error Rate', 'BACC', 'TSS','HSS','Brier Score','AUC']

"""transform into feature-target dataset"""

X,y = load_breast_cancer(return_X_y=True)

"""**decision tree**"""

dtree = ['Decision Tree']

"""find optimal tree depth by measuring cross validation accuracy

"""

depths = [1,2,3,4,5,6,7,8,9,10]
show_probabilities = True
bestdepth=0
bestmean=0
for depth in depths:
  model = DecisionTreeClassifier(max_depth=depth)

  print()
  print('depth:'+ str(depth))
  cross_val_scores = cross_val_score(model, X, y, cv=10)
  print("Cross-validation accuracy scores:")
  print(cross_val_scores)
  mean_accuracy = np.mean(cross_val_scores)
  print("Cross-validation accuracy mean = %.3f" % mean_accuracy)
  if mean_accuracy>bestmean:
    bestmean=mean_accuracy
    bestdepth=depth
print()
print('ideal depth '+str(bestdepth))
print('highest cross val mean accuracy '+str(bestmean))

"""Build Decision Tree model using best found depth and print cross validation scores based on accuracy"""

bestModel = DecisionTreeClassifier(max_depth=bestdepth)

"""print confusion matrices for each run of cross validation"""

kf = KFold(n_splits=10)
precisions=[]
recalls=[]
tnrs=[]
f1s=[]
accs=[]
errors=[]
baccs=[]
tsss=[]
hsss=[]
briers=[]
aucs=[]

for i, (train_index, test_index) in enumerate(kf.split(X)):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    bestModel.fit(X_train, y_train)
    print()
    print('fold:'+str(i))
    print(confusion_matrix(y_test, bestModel.predict(X_test)))
    true_negative, false_positive, false_negative, true_positive = confusion_matrix(y_test, bestModel.predict(X_test)).ravel()
    precision = true_positive / (true_positive + false_positive)
    recall = true_positive / (true_positive + false_negative)
    tnr = true_negative / (true_negative + false_positive)
    f1_score = 2 * (precision * recall) / (precision + recall)
    accuracy=(true_positive + true_negative)/(true_positive+false_positive+true_negative+false_negative)
    error_rate= (false_positive+false_negative)/(true_positive+false_positive+true_negative+false_negative)
    BACC = (tnr+recall)/2
    TSS=recall-(false_positive/(true_negative+false_positive))
    HSS = 2*(true_positive*true_negative - false_positive*false_negative)/((true_positive+false_negative)*(false_negative+true_negative)+(true_positive+false_positive)*(false_positive+true_negative))
    auc=roc_auc_score(y_test, bestModel.predict(X_test), average=None)
    brier = brier_score_loss(y_test, bestModel.predict(X_test))

    print(f"Precision: {precision:.2f}")
    print(f"Recall (True Positive Rate): {recall:.2f}")
    print(f"True Negative Rate: {tnr:.2f}")
    print(f"F1 Score: {f1_score:.2f}")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Error Rate: {error_rate:.2f}")
    print(f"BACC: {BACC:.2f}")
    print(f"TSS: {TSS:.2f}")
    print(f"HSS: {HSS:.2f}")
    print(f"Brier Score: {brier:.2f}")
    print(f"AUC: {auc:.2f}")

    precisions.append(precision)
    recalls.append(recall)
    tnrs.append(tnr)
    f1s.append(f1_score)
    accs.append(accuracy)
    errors.append(error_rate)
    baccs.append(BACC)
    tsss.append(TSS)
    hsss.append(HSS)
    briers.append(brier)
    aucs.append(auc)


avgprecision=np.mean(precisions)
avgrecall=np.mean(recalls)
avgtnr=np.mean(tnr)
avgf1=np.mean(f1s)
avgacc=np.mean(accs)
avgerror=np.mean(errors)
avgbacc=np.mean(baccs)
avgtss=np.mean(tsss)
avghss=np.mean(hsss)
avgbrier=np.mean(briers)
avgauc=np.mean(aucs)

dtree.append(avgprecision)
dtree.append(avgrecall)
dtree.append(avgtnr)
dtree.append(avgf1)
dtree.append(avgacc)
dtree.append(avgerror)
dtree.append(avgbacc)
dtree.append(avgtss)
dtree.append(avghss)
dtree.append(avgbrier)
dtree.append(avgauc)

"""**Random Forest**

Build Random forest
"""

rf=['Random Forest']

from sklearn.ensemble import RandomForestClassifier

kf = KFold(n_splits=10)
precisions=[]
recalls=[]
tnrs=[]
f1s=[]
accs=[]
errors=[]
baccs=[]
tsss=[]
hsss=[]
briers=[]
aucs=[]


depths = [1,2,3,4,5,6,7,8,9,10]
show_probabilities = True
bestdepth=0
bestmean=0
for depth in depths:
  model = RandomForestClassifier(max_depth=depth, random_state=0)
  print()
  print('depth:'+ str(depth))
  cross_val_scores = cross_val_score(model, X, y, cv=10)
  print("Cross-validation accuracy scores:")
  print(cross_val_scores)
  mean_accuracy = np.mean(cross_val_scores)
  print("Cross-validation accuracy mean = %.3f" % mean_accuracy)
  if mean_accuracy>bestmean:
    bestmean=mean_accuracy
    bestdepth=depth
print()
print('ideal depth '+str(bestdepth))
print('highest cross val mean accuracy '+str(bestmean))

"""build random forest model using found best depth

"""

bestModel = RandomForestClassifier(max_depth=bestdepth)

"""print confusion matrices for each run of cross validation"""

kf = KFold(n_splits=10)
precisions=[]
recalls=[]
tnrs=[]
f1s=[]
accs=[]
errors=[]
baccs=[]
tsss=[]
hsss=[]
briers=[]
aucs=[]

for i, (train_index, test_index) in enumerate(kf.split(X)):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    bestModel.fit(X_train, y_train)
    print()
    print('fold:'+str(i))
    print(confusion_matrix(y_test, bestModel.predict(X_test)))
    true_negative, false_positive, false_negative, true_positive = confusion_matrix(y_test, bestModel.predict(X_test)).ravel()
    precision = true_positive / (true_positive + false_positive)
    recall = true_positive / (true_positive + false_negative)
    tnr = true_negative / (true_negative + false_positive)
    f1_score = 2 * (precision * recall) / (precision + recall)
    accuracy=(true_positive + true_negative)/(true_positive+false_positive+true_negative+false_negative)
    error_rate= (false_positive+false_negative)/(true_positive+false_positive+true_negative+false_negative)
    BACC = (tnr+recall)/2
    TSS=recall-(false_positive/(true_negative+false_positive))
    HSS = 2*(true_positive*true_negative - false_positive*false_negative)/((true_positive+false_negative)*(false_negative+true_negative)+(true_positive+false_positive)*(false_positive+true_negative))
    auc=roc_auc_score(y_test, bestModel.predict(X_test), average=None)
    brier = brier_score_loss(y_test, bestModel.predict(X_test))

    print(f"Precision: {precision:.2f}")
    print(f"Recall (True Positive Rate): {recall:.2f}")
    print(f"True Negative Rate: {tnr:.2f}")
    print(f"F1 Score: {f1_score:.2f}")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Error Rate: {error_rate:.2f}")
    print(f"BACC: {BACC:.2f}")
    print(f"TSS: {TSS:.2f}")
    print(f"HSS: {HSS:.2f}")
    print(f"Brier Score: {brier:.2f}")
    print(f"AUC: {auc:.2f}")

    precisions.append(precision)
    recalls.append(recall)
    tnrs.append(tnr)
    f1s.append(f1_score)
    accs.append(accuracy)
    errors.append(error_rate)
    baccs.append(BACC)
    tsss.append(TSS)
    hsss.append(HSS)
    briers.append(brier)
    aucs.append(auc)


avgprecision=np.mean(precisions)
avgrecall=np.mean(recalls)
avgtnr=np.mean(tnr)
avgf1=np.mean(f1s)
avgacc=np.mean(accs)
avgerror=np.mean(errors)
avgbacc=np.mean(baccs)
avgtss=np.mean(tsss)
avghss=np.mean(hsss)
avgbrier=np.mean(briers)
avgauc=np.mean(aucs)

rf.append(avgprecision)
rf.append(avgrecall)
rf.append(avgtnr)
rf.append(avgf1)
rf.append(avgacc)
rf.append(avgerror)
rf.append(avgbacc)
rf.append(avgtss)
rf.append(avghss)
rf.append(avgbrier)
rf.append(avgauc)

"""**Support Vector Machine**"""

suppvm=['Support Vector Machine']

from sklearn import svm
bestModel = svm.SVC()

"""Perform K Fold Cross Validation and print confusion matrix and statistics

"""

kf = KFold(n_splits=10)
precisions=[]
recalls=[]
tnrs=[]
f1s=[]
accs=[]
errors=[]
baccs=[]
tsss=[]
hsss=[]
briers=[]
aucs=[]

kf = KFold(n_splits=10)
i=1
for i, (train_index, test_index) in enumerate(kf.split(X)):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    bestModel.fit(X_train, y_train)
    print()
    print('fold:'+str(i))
    print(confusion_matrix(y_test, bestModel.predict(X_test)))
    true_negative, false_positive, false_negative, true_positive = confusion_matrix(y_test, bestModel.predict(X_test)).ravel()
    precision = true_positive / (true_positive + false_positive)
    recall = true_positive / (true_positive + false_negative)
    tnr = true_negative / (true_negative + false_positive)
    f1_score = 2 * (precision * recall) / (precision + recall)
    accuracy=(true_positive + true_negative)/(true_positive+false_positive+true_negative+false_negative)
    error_rate= (false_positive+false_negative)/(true_positive+false_positive+true_negative+false_negative)
    BACC = (tnr+recall)/2
    TSS=recall-(false_positive/(true_negative+false_positive))
    HSS = 2*(true_positive*true_negative - false_positive*false_negative)/((true_positive+false_negative)*(false_negative+true_negative)+(true_positive+false_positive)*(false_positive+true_negative))
    auc=roc_auc_score(y_test, bestModel.predict(X_test), average=None)
    brier = brier_score_loss(y_test, bestModel.predict(X_test))

    print(f"Precision: {precision:.2f}")
    print(f"Recall (True Positive Rate): {recall:.2f}")
    print(f"True Negative Rate: {tnr:.2f}")
    print(f"F1 Score: {f1_score:.2f}")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Error Rate: {error_rate:.2f}")
    print(f"BACC: {BACC:.2f}")
    print(f"TSS: {TSS:.2f}")
    print(f"HSS: {HSS:.2f}")
    print(f"Brier Score: {brier:.2f}")
    print(f"AUC: {auc:.2f}")

    precisions.append(precision)
    recalls.append(recall)
    tnrs.append(tnr)
    f1s.append(f1_score)
    accs.append(accuracy)
    errors.append(error_rate)
    baccs.append(BACC)
    tsss.append(TSS)
    hsss.append(HSS)
    briers.append(brier)
    aucs.append(auc)


avgprecision=np.mean(precisions)
avgrecall=np.mean(recalls)
avgtnr=np.mean(tnr)
avgf1=np.mean(f1s)
avgacc=np.mean(accs)
avgerror=np.mean(errors)
avgbacc=np.mean(baccs)
avgtss=np.mean(tsss)
avghss=np.mean(hsss)
avgbrier=np.mean(briers)
avgauc=np.mean(aucs)

suppvm.append(avgprecision)
suppvm.append(avgrecall)
suppvm.append(avgtnr)
suppvm.append(avgf1)
suppvm.append(avgacc)
suppvm.append(avgerror)
suppvm.append(avgbacc)
suppvm.append(avgtss)
suppvm.append(avghss)
suppvm.append(avgbrier)
suppvm.append(avgauc)

"""**Deep Learning**"""

! pip install keras

from keras import Sequential
from keras.layers import Dense, GRU
from sklearn.metrics import brier_score_loss

gru=['Deep Learning GRU']

"""Build Model"""

bestModel = Sequential()
bestModel.add(GRU(2, input_shape=(X_train.shape[1], 1)))
bestModel.add(Dense(1, activation="sigmoid"))
bestModel.compile(optimizer="adam", loss="binary_crossentropy", metrics=["acc"])

"""iterate through k folds"""

kf = KFold(n_splits=10)
precisions=[]
recalls=[]
tnrs=[]
f1s=[]
accs=[]
errors=[]
baccs=[]
tsss=[]
hsss=[]
briers=[]
aucs=[]

kf = KFold(n_splits=10)
i=1
for i, (train_index, test_index) in enumerate(kf.split(X)):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    bestModel.fit(X_train, y_train)
    print()
    print('fold:'+str(i))
    predictions = bestModel.predict(X_test).round()
    print(confusion_matrix(y_test, predictions))
    true_negative, false_positive, false_negative, true_positive = confusion_matrix(y_test, predictions).ravel()
    precision = true_positive / (true_positive + false_positive)
    recall = true_positive / (true_positive + false_negative)
    tnr = true_negative / (true_negative + false_positive)
    f1_score = 2 * (precision * recall) / (precision + recall)
    accuracy=(true_positive + true_negative)/(true_positive+false_positive+true_negative+false_negative)
    error_rate= (false_positive+false_negative)/(true_positive+false_positive+true_negative+false_negative)
    BACC = (tnr+recall)/2
    TSS=recall-(false_positive/(true_negative+false_positive))
    HSS = 2*(true_positive*true_negative - false_positive*false_negative)/((true_positive+false_negative)*(false_negative+true_negative)+(true_positive+false_positive)*(false_positive+true_negative))
    brier = brier_score_loss(y_test, predictions)
    auc=roc_auc_score(y_test, predictions, average=None)

    print(f"Precision: {precision:.2f}")
    print(f"Recall (True Positive Rate): {recall:.2f}")
    print(f"True Negative Rate: {tnr:.2f}")
    print(f"F1 Score: {f1_score:.2f}")
    print(f"Accuracy: {accuracy:.2f}")
    print(f"Error Rate: {error_rate:.2f}")
    print(f"BACC: {BACC:.2f}")
    print(f"TSS: {TSS:.2f}")
    print(f"HSS: {HSS:.2f}")
    print(f"Brier Score: {brier:.2f}")
    print(f"AUC: {auc:.2f}")

    precisions.append(precision)
    recalls.append(recall)
    tnrs.append(tnr)
    f1s.append(f1_score)
    accs.append(accuracy)
    errors.append(error_rate)
    baccs.append(BACC)
    tsss.append(TSS)
    hsss.append(HSS)
    briers.append(brier)
    aucs.append(auc)


avgprecision=np.mean(precisions)
avgrecall=np.mean(recalls)
avgtnr=np.mean(tnr)
avgf1=np.mean(f1s)
avgacc=np.mean(accs)
avgerror=np.mean(errors)
avgbacc=np.mean(baccs)
avgtss=np.mean(tsss)
avghss=np.mean(hsss)
avgbrier=np.mean(briers)
avgauc=np.mean(aucs)

gru.append(avgprecision)
gru.append(avgrecall)
gru.append(avgtnr)
gru.append(avgf1)
gru.append(avgacc)
gru.append(avgerror)
gru.append(avgbacc)
gru.append(avgtss)
gru.append(avghss)
gru.append(avgbrier)
gru.append(avgauc)

"""create table"""

df = pd.DataFrame(list(zip(metricnames, dtree, rf, suppvm, gru)))
print(df)